{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80aca974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "to_numpy = lambda x: x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8697f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICLTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=100, val_dim=1, embed_dim=256, num_heads=8, num_layers=12,\n",
    "                 image_size=16, patch_size=4, max_seq_len=128):\n",
    "        # max_seq_len corresponds to 2k, so k x_k samples and k f(x_k), and then one more for x_query\n",
    "        super().__init__()\n",
    "\n",
    "        self.grid_h = image_size // patch_size\n",
    "        self.grid_w = image_size // patch_size\n",
    "        self.num_patches = self.grid_h * self.grid_w\n",
    "\n",
    "        self.time_embed = nn.Parameter(torch.randn(1, max_seq_len * 2, embed_dim))\n",
    "\n",
    "        self.val_dim = val_dim\n",
    "        self.value_proj = nn.Linear(val_dim, embed_dim)\n",
    "\n",
    "        self.fc_in = nn.Linear(input_dim, 256)\n",
    "        # decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "        # self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        config = GPT2Config(\n",
    "            n_positions=2 * max_seq_len,\n",
    "            n_embd=embed_dim,\n",
    "            n_layer=num_layers,\n",
    "            n_head=num_heads,\n",
    "            resid_pdrop=0.0,\n",
    "            embd_pdrop=0.0,\n",
    "            attn_pdrop=0.0,\n",
    "            use_cache=False,\n",
    "        )\n",
    "        self.transformer = GPT2Model(config)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_dim, val_dim)\n",
    "\n",
    "    def forward(self, x_in, values):\n",
    "        x_embeddings = self.fc_in(x_in)\n",
    "        B, T, _ = x_embeddings.shape\n",
    "        \n",
    "        val_embeds = self.value_proj(values)\n",
    "        embeddings = self.interleave(x_embeddings, val_embeds)\n",
    "        inds = torch.arange(T).to(device)\n",
    "        # embeddings = embeddings + self.time_embed[:, :2 * T]\n",
    "\n",
    "        # mask = nn.Transformer.generate_square_subsequent_mask(2 * T).to(embeddings.device)\n",
    "        # memory = torch.zeros(B, 1, embeddings.shape[-1]).to(embeddings.device)\n",
    "        # x = self.transformer(tgt=embeddings, memory=memory, tgt_mask=mask)\n",
    "        x = self.transformer(inputs_embeds=embeddings).last_hidden_state\n",
    "        predictions = self.fc_out(x)\n",
    "\n",
    "        return predictions[:, ::2, 0][:, inds]\n",
    "    \n",
    "    def interleave(self, xs, ys):\n",
    "        B, T, D = xs.shape\n",
    "        stacked = torch.stack((xs, ys), dim=2)  # [B, T, 2, D]\n",
    "        interleaved = stacked.view(B, 2 * T, D)\n",
    "        return interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0753bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=100, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        layers = [m for m in self.modules() if isinstance(m, (nn.Linear, nn.Conv2d))]\n",
    "        for i, m in enumerate(layers):\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                if i < len(layers) - 1:\n",
    "                    nn.init.normal_(m.weight)\n",
    "                else:\n",
    "                    nn.init.normal_(m.weight, std=2/self.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "695873f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: 5.224421501159668, 6.439401626586914\n",
      "200: 4.115876197814941, 4.443578720092773\n",
      "300: 4.406532287597656, 3.1530070304870605\n",
      "400: 4.095528602600098, 3.0782384872436523\n",
      "500: 3.84342622756958, 3.2245068550109863\n",
      "600: 3.4031782150268555, 3.1673550605773926\n",
      "700: 4.011458873748779, 3.2145540714263916\n",
      "800: 3.410419225692749, 2.5860767364501953\n",
      "900: 2.815985918045044, 3.127397060394287\n",
      "1000: 2.7383556365966797, 1.4801092147827148\n",
      "1100: 2.7133703231811523, 1.680066466331482\n",
      "1200: 2.337367057800293, 1.5427826642990112\n",
      "1300: 2.147916793823242, 0.8538510799407959\n",
      "1400: 1.8265982866287231, 0.4219081401824951\n",
      "1500: 2.0315144062042236, 0.6993225812911987\n",
      "1600: 1.6928085088729858, 0.3308414816856384\n",
      "1700: 1.65704345703125, 0.26988545060157776\n",
      "1800: 1.9510554075241089, 0.3630797564983368\n",
      "1900: 1.5730009078979492, 0.176939457654953\n",
      "2000: 1.6106585264205933, 0.2578014135360718\n",
      "2100: 3.820971727371216, 3.325739860534668\n",
      "2200: 3.862192153930664, 1.2951825857162476\n",
      "2300: 2.6715681552886963, 1.2335604429244995\n",
      "2400: 2.0933279991149902, 0.3814132809638977\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m xs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_max\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m xs[:, :, d_cur:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m ws \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m1\u001b[39m, d_max)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 500_000\n",
    "batch_size = 64\n",
    "d_max = 100\n",
    "d_cur = 5\n",
    "n_samples = 2 * d_cur + 1\n",
    "losses = []\n",
    "final_losses = []\n",
    "transformer = ICLTransformer(d_max).to(device)\n",
    "optim = torch.optim.AdamW(transformer.parameters(), 1e-4)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    if epoch % 2000 == 0:\n",
    "        d_cur += 1\n",
    "        n_samples = 2 * d_cur + 1\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'{epoch}: {losses[-1]}, {final_losses[-1]}')\n",
    "    xs = torch.randn(batch_size, n_samples, d_max).to(device)\n",
    "    xs[:, :, d_cur:] = 0\n",
    "    ws = torch.randn(batch_size, 1, d_max).to(device)\n",
    "    ys = (ws * xs).sum(-1, keepdim=True)\n",
    "    y_preds = transformer(xs, ys)\n",
    "    y_targets = ys.squeeze(-1)\n",
    "    loss = F.mse_loss(y_preds, y_targets)\n",
    "    final_loss = F.mse_loss(y_preds[:, -1], y_targets[:, -1])\n",
    "    losses.append(loss.item())\n",
    "    final_losses.append(final_loss.item())\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b07b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: 0.048953838646411896, 0.033080246299505234\n",
      "200: 0.029762040823698044, 0.029734712094068527\n",
      "300: 0.030237438157200813, 0.02668357826769352\n",
      "400: 0.035440947860479355, 0.021727513521909714\n",
      "500: 0.04698522761464119, 0.05671188235282898\n",
      "600: 0.04262048751115799, 0.021662253886461258\n",
      "700: 0.033532582223415375, 0.040008530020713806\n",
      "800: 0.03731117397546768, 0.021442417055368423\n",
      "900: 0.03133833408355713, 0.03280448913574219\n",
      "1000: 0.031016496941447258, 0.02640456147491932\n",
      "1100: 0.031058307737112045, 0.020216336473822594\n",
      "1200: 0.03279753029346466, 0.025422774255275726\n",
      "1300: 0.03680151700973511, 0.04315032437443733\n",
      "1400: 0.052755169570446014, 0.03241295367479324\n",
      "1500: 0.04150332137942314, 0.05401502922177315\n",
      "1600: 0.03237580880522728, 0.02893785759806633\n",
      "1700: 0.04033122584223747, 0.04666707664728165\n",
      "1800: 0.046846844255924225, 0.09029857814311981\n",
      "1900: 0.026266159489750862, 0.016595646739006042\n",
      "2000: 0.03872684761881828, 0.018839705735445023\n",
      "2100: 0.055760517716407776, 0.02602071687579155\n",
      "2200: 0.0433298721909523, 0.037413179874420166\n"
     ]
    }
   ],
   "source": [
    "# n_epochs = 500_000\n",
    "# batch_size = 64\n",
    "# d_max = 100\n",
    "# d_cur = 5\n",
    "# n_samples = 5 * d_cur + 1\n",
    "# losses = []\n",
    "# final_losses = []\n",
    "# transformer = ICLTransformer(d_max).to(device)\n",
    "# optim = torch.optim.AdamW(transformer.parameters(), 1e-4)\n",
    "\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "#     if epoch % 2000 == 0:\n",
    "#         d_cur += 1\n",
    "#         n_samples = 5 * d_cur + 1\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'{epoch}: {losses[-1]}, {final_losses[-1]}')\n",
    "#     xs = torch.randn(batch_size, n_samples, d_max).to(device)\n",
    "#     xs[:, :, d_cur:] = 0\n",
    "#     ws_1 = torch.randn(batch_size, 1, d_max).to(device)\n",
    "#     ws_2 = torch.randn(batch_size, 1, d_max).to(device) * (2 / d_max)**0.5\n",
    "#     ys = (ws_2 * F.relu(ws_1 * xs)).sum(-1, keepdim=True)\n",
    "#     y_preds = transformer(xs, ys)\n",
    "#     y_targets = ys\n",
    "#     loss = F.mse_loss(y_preds, y_targets)\n",
    "#     final_loss = F.mse_loss(y_preds[:, -1], y_targets[:, -1])\n",
    "#     losses.append(loss.item())\n",
    "#     final_losses.append(final_loss.item())\n",
    "#     optim.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983db28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5038, -1.4061, -0.4550, -1.7284,  0.2009, -0.8888, -1.2016,  2.2628,\n",
      "         0.2417,  3.3320,  2.4030, -2.4408,  2.5768,  0.3298,  1.2715,  1.2522,\n",
      "        -1.9192,  2.8138, -1.6523,  3.4755, -5.4497, -0.6134,  0.7242, -1.3211,\n",
      "         3.4456,  2.2038,  0.3460,  0.7415,  0.3938,  0.9828,  1.4486, -5.0805,\n",
      "         1.9532, -0.9781,  0.9409,  0.4682,  0.6703, -5.7165,  0.7405, -0.6933,\n",
      "         2.5001, -1.4460,  1.5262,  2.3915,  1.1792, -5.3400, -0.0508, -2.6798,\n",
      "        -1.3692, -3.3743,  1.2782,  3.0200, -1.7513, -0.6687,  2.6614, -1.3479,\n",
      "         2.2771,  1.7686,  3.2906, -1.9842,  2.5961, -0.4701, -2.1348,  1.6573],\n",
      "       device='cuda:1', grad_fn=<SelectBackward0>)\n",
      "tensor([-1.6063,  0.6132, -0.0369, -2.3265, -2.0652, -3.0482, -1.1844, -1.6541,\n",
      "        -0.7453,  3.5628,  2.2337, -0.4224,  3.4141, -0.1282,  1.3983,  1.4424,\n",
      "        -2.6016,  0.8868, -1.6059,  3.2607, -7.0381, -0.6906,  0.6043, -0.4372,\n",
      "         2.9834,  2.7182,  0.0267,  0.3544, -3.5444,  1.9392,  2.3086, -7.9013,\n",
      "         2.0224, -0.3174,  0.1260, -1.0457,  4.0398, -8.3959,  2.5113, -1.2555,\n",
      "         1.7785, -4.0827,  2.2237,  3.3049, -0.0661, -6.0194,  0.1797, -3.5542,\n",
      "        -2.1384, -2.3163,  5.5705,  4.3379, -1.1607,  0.6767,  3.8210, -3.5867,\n",
      "         1.0119,  3.2920,  2.1898, -3.6530, -0.8035, -0.6248, -3.7352,  3.2745],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "print(y_preds[:, -1])\n",
    "print(y_targets[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f1c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs182",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
