{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80aca974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "to_numpy = lambda x: x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8697f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ICLTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=100, val_dim=1, embed_dim=256, num_heads=8, num_layers=12,\n",
    "                 image_size=16, patch_size=4, max_seq_len=128):\n",
    "        # max_seq_len corresponds to 2k, so k x_k samples and k f(x_k), and then one more for x_query\n",
    "        super().__init__()\n",
    "\n",
    "        self.grid_h = image_size // patch_size\n",
    "        self.grid_w = image_size // patch_size\n",
    "        self.num_patches = self.grid_h * self.grid_w\n",
    "\n",
    "        self.time_embed = nn.Parameter(torch.randn(1, max_seq_len * 2, embed_dim))\n",
    "\n",
    "        self.val_dim = val_dim\n",
    "        self.value_proj = nn.Linear(val_dim, embed_dim)\n",
    "\n",
    "        self.fc_in = nn.Linear(input_dim, 256)\n",
    "        # decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "        # self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        config = GPT2Config(\n",
    "            n_positions=2 * max_seq_len,\n",
    "            n_embd=embed_dim,\n",
    "            n_layer=num_layers,\n",
    "            n_head=num_heads,\n",
    "            resid_pdrop=0.0,\n",
    "            embd_pdrop=0.0,\n",
    "            attn_pdrop=0.0,\n",
    "            use_cache=False,\n",
    "        )\n",
    "        self.transformer = GPT2Model(config)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_dim, val_dim)\n",
    "\n",
    "    def forward(self, x_in, values):\n",
    "        x_embeddings = self.fc_in(x_in)\n",
    "        B, T, _ = x_embeddings.shape\n",
    "        \n",
    "        val_embeds = self.value_proj(values)\n",
    "        embeddings = self.interleave(x_embeddings, val_embeds)\n",
    "        inds = torch.arange(T).to(device)\n",
    "        # embeddings = embeddings + self.time_embed[:, :2 * T]\n",
    "\n",
    "        # mask = nn.Transformer.generate_square_subsequent_mask(2 * T).to(embeddings.device)\n",
    "        # memory = torch.zeros(B, 1, embeddings.shape[-1]).to(embeddings.device)\n",
    "        # x = self.transformer(tgt=embeddings, memory=memory, tgt_mask=mask)\n",
    "        x = self.transformer(inputs_embeds=embeddings).last_hidden_state\n",
    "        predictions = self.fc_out(x)\n",
    "\n",
    "        return predictions[:, ::2, 0][:, inds] \n",
    "    \n",
    "    def interleave(self, xs, ys):\n",
    "        B, T, D = xs.shape\n",
    "        stacked = torch.stack((xs, ys), dim=2)  # [B, T, 2, D]\n",
    "        interleaved = stacked.view(B, 2 * T, D)\n",
    "        return interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695873f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74477/3368534940.py:23: UserWarning: Using a target size (torch.Size([64, 11, 1, 1])) that is different to the input size (torch.Size([64, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_preds, y_targets)\n",
      "/tmp/ipykernel_74477/3368534940.py:24: UserWarning: Using a target size (torch.Size([64, 1, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  final_loss = F.mse_loss(y_preds[:, -1], y_targets[:, -1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: 4.592724800109863, 4.225244522094727\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m xs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, n_samples, d_max)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m xs[:, :, d_cur:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 19\u001b[0m ws \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_max\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m ys \u001b[38;5;241m=\u001b[39m (ws \u001b[38;5;241m*\u001b[39m xs)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m transformer(xs, ys)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 500_000\n",
    "batch_size = 64\n",
    "d_max = 100\n",
    "d_cur = 5\n",
    "n_samples = 2 * d_cur + 1\n",
    "losses = []\n",
    "final_losses = []\n",
    "transformer = ICLTransformer(d_max).to(device)\n",
    "optim = torch.optim.AdamW(transformer.parameters(), 1e-4)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    if epoch % 2000 == 0:\n",
    "        d_cur += 1\n",
    "        n_samples = 2 * d_cur + 1\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'{epoch}: {losses[-1]}, {final_losses[-1]}')\n",
    "    xs = torch.randn(batch_size, n_samples, d_max).to(device)\n",
    "    xs[:, :, d_cur:] = 0\n",
    "    ws = torch.randn(batch_size, 1, d_max).to(device)\n",
    "    ys = (ws * xs).sum(-1, keepdim=True)\n",
    "    y_preds = transformer(xs, ys)\n",
    "    y_targets = ys.squeeze(-1)\n",
    "    loss = F.mse_loss(y_preds, y_targets)\n",
    "    final_loss = F.mse_loss(y_preds[:, -1], y_targets[:, -1])\n",
    "    losses.append(loss.item())\n",
    "    final_losses.append(final_loss.item())\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983db28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs182",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
